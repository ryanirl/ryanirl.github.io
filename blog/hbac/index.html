<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/png" href="/assets/favicon_square.png"><meta name="description" content="Ryan Peters is a student and researcher, working at the intersection of machine learning, neuroscience, and healthcare."><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet"><meta name="generator" content="Astro v4.12.2"><title>Harmful Brain Activity Classification</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"><script type="text/partytown" defer data-domain="ryanirl.com" src="https://plausible.io/js/script.js"></script><link rel="stylesheet" href="/_astro/about.CEAy0-kL.css">
<style>.citation-tooltip[data-astro-cid-eqo3tkq5]{display:none;position:absolute;background-color:var(--tooltip-bg);color:var(--tooltip-text);border:1px solid var(--tooltip-border);padding:10px;border-radius:4px;font-size:.9em;max-width:500px;z-index:1000;box-shadow:0 2px 5px #0003}.citation-tooltip[data-astro-cid-eqo3tkq5] a[data-astro-cid-eqo3tkq5]{color:var(--tooltip-link);text-decoration:underline}.references-list[data-astro-cid-eqo3tkq5]{padding-left:20px}:root{--tooltip-bg: #f9f9f9;--tooltip-text: #333;--tooltip-border: #ccc;--tooltip-link: #0000EE}:root.dark{--tooltip-bg: #2a2a2a;--tooltip-text: #e0e0e0;--tooltip-border: #555;--tooltip-link: #879eff}.citation-ref[data-astro-cid-tbsazfha]{cursor:pointer;color:var(--citation-color)}:root{--citation-color: #6d6dbf}:root.dark{--citation-color: #8089b4}.break-words[data-astro-cid-xj2uyz6m]{word-wrap:break-word;overflow-wrap:break-word;-webkit-hyphens:auto;hyphens:auto}.break-all[data-astro-cid-xj2uyz6m]{word-break:break-all}.grid-0[data-astro-cid-whmuftws]{flex:none}@media (min-width: 768px){.grid-0[data-astro-cid-whmuftws]{width:12rem}}.grid-1[data-astro-cid-whmuftws]{flex:0 1 auto}@media (min-width: 768px){.grid-1[data-astro-cid-whmuftws]{width:73rem}}.grid-2[data-astro-cid-whmuftws]{flex:0 1 auto}@media (min-width: 768px){.grid-2[data-astro-cid-whmuftws]{width:0rem}}
</style>
<link rel="stylesheet" href="/_astro/gamblers_ruin.BSsibEFz.css">
<style>.light-text[data-astro-cid-oemx5le4]{display:none}.dark-text[data-astro-cid-oemx5le4],.dark .light-text[data-astro-cid-oemx5le4]{display:inline}.dark .dark-text[data-astro-cid-oemx5le4]{display:none}
</style><script type="module">const l=JSON.parse(document.getElementById("citations-data").textContent),o=document.getElementById("citationTooltip");let a;function u(t,e){clearTimeout(a),o.innerHTML=`
      <strong>${e.title}</strong><br>
      ${e.authors.join(", ")} (${e.year})
      ${e.url?`<br><a href="${e.url}" target="_blank" rel="noopener noreferrer">View source</a>`:""}
    `,o.style.display="block";const i=o.getBoundingClientRect(),s=window.innerWidth,r=window.innerHeight;let n=t.pageX+10;n+i.width>s&&(n=Math.max(0,s-i.width));let d=t.pageY+10;d+i.height>r&&(d=Math.max(0,t.pageY-i.height-10)),o.style.left=`${n}px`,o.style.top=`${d}px`}function c(){a=setTimeout(()=>{o.style.display="none"},300)}document.body.addEventListener("mouseover",t=>{const e=t.target;if(e.closest(".citation-ref")){const i=e.closest(".citation-ref"),s=parseInt(i.dataset.citationId),r=l.find(n=>n.id===s);r&&u(t,r)}});document.body.addEventListener("mouseout",t=>{t.target.closest(".citation-ref")&&c()});o.addEventListener("mouseover",()=>{clearTimeout(a)});o.addEventListener("mouseout",c);
</script>
<script>!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=Object.assign(w[p]||{},{"lib":"/~partytown/","debug":false});c[f]=(c[f]||[]).concat(["dataLayer.push"])})(window,'partytown','forward');/* Partytown 0.10.2 - MIT builder.io */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,d,l,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(l=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(d=setTimeout(v,1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(e){p=r.createElement(e?"script":"iframe"),t._pttab=Date.now(),e||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(e?"atomics.js?v=0.10.2":"sandbox-sw.html?"+t._pttab),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<l.length;n++)(o=r.createElement("script")).innerHTML=l[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(d)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);;(e=>{e.addEventListener("astro:before-swap",e=>{let r=document.body.querySelector("iframe[src*='/~partytown/']");e.newDocument.body.append(r)})})(document);</script></head> <body class="bg-[#fafbfd] dark:bg-[#1d1e20] flex flex-col min-h-screen  tracking-[-0.01rem]"> <nav class="sticky bg-[#fafbfd] dark:bg-[#1d1e20] top-0 z-50" data-astro-cid-whmuftws> <div class="max-w-[85rem] mx-auto w-full pt-4 pb-0 mb-1" data-astro-cid-whmuftws> <div class="md:px-6 px-4 flex items-center" data-astro-cid-whmuftws> <div class="grid-0" data-astro-cid-whmuftws> <a href="/" class="md:inline hidden text-nowrap text-black font-medium text-[1.34rem] dark:text-gray-200 tracking-[-0.02rem]" data-astro-cid-whmuftws>
Ryan Peters
</a> </div> <div class="grid-1 flex justify-start w-[75rem]" data-astro-cid-whmuftws> <a href="/" class="md:ml-0 decoration-2 underline-offset-4 flex items-center dark:hover:text-gray-400 hover:text-gray-500 transition-colors duration-200 text-lg font-base text-gray-900 dark:text-gray-300" data-astro-cid-whmuftws>
Blog
</a> <a href="/news" class="ml-6 decoration-2 underline-offset-4 flex items-center dark:hover:text-gray-400 hover:text-gray-500 transition-colors duration-200 text-lg font-base text-gray-900 dark:text-gray-300" data-astro-cid-whmuftws>
News
</a> <a href="/about" class="ml-6 decoration-2 underline-offset-4 flex items-center dark:hover:text-gray-400 hover:text-gray-500 transition-colors duration-200 text-lg font-base text-gray-900 dark:text-gray-300" data-astro-cid-whmuftws>
About
</a> </div> <div class="grid-2 flex justify-end w-[15rem]" data-astro-cid-whmuftws> <button id="themeToggle" class="theme-toggle" data-astro-cid-oemx5le4> <span class="light-text text-gray-300 text-lg hover:text-gray-400 transition-colors duration-200 " data-astro-cid-oemx5le4>Light</span> <span class="dark-text text-lg hover:text-gray-500 transition-colors duration-200" data-astro-cid-oemx5le4>Dark</span> </button>  <script>
  const theme = (() => {
    if (typeof localStorage !== 'undefined' && localStorage.getItem('theme')) {
      return localStorage.getItem('theme');
    }
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
      return 'dark';
    }
    return 'light';
  })();

  if (theme === 'light') {
    document.documentElement.setAttribute("data-mode", 'light');
    document.documentElement.classList.remove('dark');
  } else {
    document.documentElement.setAttribute("data-mode", 'dark');
    document.documentElement.classList.add('dark');
  }

  window.localStorage.setItem('theme', theme);

  const handleToggleClick = () => {
    const element = document.documentElement;
    element.classList.toggle("dark");
    const isDark = element.classList.contains("dark");
    localStorage.setItem("theme", isDark ? "dark" : "light");
    element.setAttribute("data-mode", isDark ? "dark" : "light");
  }

  document.getElementById("themeToggle").addEventListener("click", handleToggleClick);
</script> </div> </div> </div> </nav>  <div class="max-w-3xl mx-auto w-full px-4"> <div class="text-gray-500 dark:text-gray-400 font-base pt-16 pb-2"> May 6, 2024 </div> <h1 class="text-gray-900 dark:text-gray-100 pr-5 text-3xl font-semibold mb-2">Harmful Brain Activity Classification</h1> <div> <span class="author text-gray-800 dark:text-gray-300"> <b>Authors:</b> </span>
          <span class="author text-gray-500 dark:text-gray-400 font-base"> Ryan Peters<sup>*</sup>  </span> </div> <div class="text-gray-500 dark:text-gray-400 font-base text-sm pt-4"> <span>* Core Contributor;</span> <span class="pl-4">Correspondence to ryanirl@icloud.com;</span> </div> <div data-astro-cid-xj2uyz6m> <div class="text-base max-w-3xl prose dark:prose-invert text-gray-900 dark:text-gray-200 my-8 break-words" data-astro-cid-xj2uyz6m>  <p>The classification of harmful brain activity [<span class="citation-ref" data-citation-id="1" data-astro-cid-tbsazfha>1</span>] [<span class="citation-ref" data-citation-id="2" data-astro-cid-tbsazfha>2</span>] [<span class="citation-ref" data-citation-id="3" data-astro-cid-tbsazfha>3</span>] [<span class="citation-ref" data-citation-id="4" data-astro-cid-tbsazfha>4</span>]
is one of the first steps in the
effective diagnosis and treatment of neurological disorders [<span class="citation-ref" data-citation-id="5" data-astro-cid-tbsazfha>5</span>] [<span class="citation-ref" data-citation-id="6" data-astro-cid-tbsazfha>6</span>]
Today, electroencephalograms (EEGs) are used as non-invasive tests to
record the electrical activity of the brain to detect various types of
brain activity, harmful or otherwise. Although, manual labeling of EEG data
is error-prone, subjective, and time consuming. For example, consider
the scenario in which a patient undergoes a 24-hour EEG monitoring session
resulting in a <em>large</em> amount of data being recorded across multiple electrode
channels. Now, imagine a neurologist having to manually review all of this data,
looking for subtle patterns that might indicate a seizure or other abnormal
brain activity. It’s a tedious task, to say the least, and one that’s prone to
fatigue, subjectivity, and error. Therefore, automation of this task is highly
desirable to both reduce labeling error and speed up the labeling process.
Doing so could improve the quality of life for both care teams and patients, and
potentially even advance drug development.</p>
<p>A recent Kaggle competition, hosted by Harvard Medical School [<span class="citation-ref" data-citation-id="7" data-astro-cid-tbsazfha>7</span>],
challenged its competitors to do just this. That is, develop a model to classify
various types of harmful brain activity, such as seizures, given EEG recordings
from human patients. The dataset provided in this competition contained 6 types
of harmful brain activity that were labeled by a varying number of expert
annotators. In this article, we present our solution that we developed for this
competition. We demonstrate that it was able to achieve 0.2625 ± 0.0233
KL-Divergence on high-quality samples, an AUC of 0.9274 ± 0.0272 on idealized
samples, and ranked 60th out of 2767 other teams. Furthermore, we claim that
our model makes minimal assumptions about the temporal length of the EEG, the
number and spatial orientation of the EEG channels, and can be extended to
include new modalities of data.</p>
<p>In the first section, we give a brief introduction to the background
information needed to understand the task at hand. In the subsequent sections,
we detail the competition data, and give an explanation of our solution. Finally,
we give a discussion on the limitations of our model, talk about the competition
itself, and mention some work on model interpretability. All of the code for
this project is fully open source, reproducible, and can be found on
<a href="https://github.com/ryanirl/hbac" target="_blank">GitHub</a>.</p>
<h2 id="background">Background</h2>
<p>An electroencephalogram (EEG) is a non-invasive tool used to record electrical
activity in the brain through electrodes that are placed on the scalp. These
electrodes detect small voltage fluctuations resulting from ionic currents
within the brain. Opposed to other neural recording devices, such as calcium
imaging [<span class="citation-ref" data-citation-id="8" data-astro-cid-tbsazfha>8</span>] [<span class="citation-ref" data-citation-id="9" data-astro-cid-tbsazfha>9</span>], EEGs don’t have the resolution to capture the activity of
single cells, but instead represent the summation of activity from large
populations of neurons.</p>
<center><figure><img src="/img/hbac/eeg_electrodes.png" alt="Alt Text" style="width: 80%;"/><figcaption style="text-align:center;"><p><b>Figure 1:</b> Graphical illustration of the EEG electrodes on the surface of the scalp.</p></figcaption></figure></center>
<p>The mechanism by which electrodes record ionic currents rely on volume
conduction, the process by which electrical potentials are able to conduct (or
travel) throughout tissue. When excitatory or inhibitory neurons fire (during
an action potential), the extracellular voltage around the neurons change, and
electrical fields are generated that propagate throughout the rest of the
brain tissue. When these fields reach the surface of the scalp, and the adjacent
electrodes, we are able to record the difference in potential created. Most of
the signal recorded by EEGs is thought to come from pyramidal neurons in layers
3 and 5 of the cerebral cortex [<span class="citation-ref" data-citation-id="10" data-astro-cid-tbsazfha>10</span>].</p>
<p>The placement of these electrodes on the scalp has been standardized, and
there are a few commonly used layouts that we will cover. The most commonly
used layout (or system) is the international 10-20 system (<strong>Figure 2</strong>). There are
also the 10-10 and 10-5 systems, which are supersets of the 10-20 system that
enable higher-density recordings.</p>
<center><figure><img src="/img/hbac/system_10_20.png" width="50%" style="background-color: white;"/><figcaption><p><b>Figure 2:</b> An illustration of the 10-20 system for EEG electrode
placement. Notice that each electrode placement site
has a letter to identify the lobe, or area of the brain, that it is located near:
pre-frontal (Fp), frontal (F), temporal (T), parietal (P), occipital (O),
and central (C).</p></figcaption></figure></center>
<p>When analyzing EEG signals, the raw signal from individual electrode channels
are often processed to create what are called “montages”. A montage is a
specific arrangement or combination of the electrode channels that helps
highlight certain features of the underlying brain activity.
One method of creating montages is through the computation of “bipolar
channels.” These are created by subtracting the signal of one electrode from
another. For example, a bipolar channel might be represented as Fp1-F7
(<strong>Figure 3</strong>), which means the signal from electrode F7 is subtracted from Fp1.
This is helpful because it allows us to treat F7 as an estimate of the baseline
activity for Fp1, and the process of subtracting F7 from Fp1 helps isolate the
non-baseline activity from Fp1. Interested readers might consider reading a
more detailed post about montages
<a href="https://www.learningeeg.com/montages-and-technical-components" target="_blank">here</a>.</p>
<p>It’s often that the primary <em>signal</em> we care about are the individual
frequencies or oscillations of that signal. For this reason, you will often here
about common frequency bands of interest, such as:</p>
<ul>
<li>Delta (0.5-4 Hz)</li>
<li>Theta (4-8 Hz)</li>
<li>Alpha (8-13 Hz)</li>
<li>Beta (13-30 Hz)</li>
<li>Gamma (&gt;30 Hz)</li>
</ul>
<p>There has been a lot of research around these frequency bands, linking them to
various patterns of behavior. For example, delta activity is commonly seen in
deep sleep states [<span class="citation-ref" data-citation-id="11" data-astro-cid-tbsazfha>11</span>].</p>
<h2 id="data">Data</h2>
<p>For this competition, we were provided 106,800 labeled samples of data. At first
glance, each sample consists of three modalities of data in the form of four
spectrograms, the EEG signal from each electrode, and an EKG signal. These
samples were given in the form of sections of 17,089 EEG/EKG recordings and
11,138 spectrograms. A single sample of data can be seen in <strong>Figure 3</strong>.</p>
<center><figure><img src="/img/hbac/seizure_example.png" width="90%" style="background-color: white;"/><figcaption><p><b>Figure 3:</b> A single sample of data given for this competition. Notice
that we were given 4 spectrograms (LL, RL, LP, RP) from the banana montage,
EEG signals for each electrode (bipolar channels shown in above), and an EKG
signal.</p></figcaption></figure></center>
<p>The specifics of each modality are listed below.</p>
<ul>
<li><strong>2D Spectrograms</strong>: We were provided four spectrograms for each sample,
corresponding to the different <em>chains</em> of the double banana bipolar montage:
left lateral (LL), right lateral (RL), left parasagittal (LP), and right
parasagittal (RP). These spectrograms were computed from a 10-minute long
EEG signal, contained frequencies up to 20 Hz.</li>
<li><strong>1D EEG Signals</strong>: In addition to the 4 spectrograms, we were given 50 second
long EEG signals recorded from electrodes placed in the 10-20 system.
The recordings were sampled at 200 Hz, and were taken from the center of the
larger 10-minute window that the spectrograms were computed from.</li>
<li><strong>EKG Signal</strong>: An electrocardiogram (EKG) signal was also provided, at the
same frequency and length of the EEG signal.</li>
</ul>
<p>Looking at the data, the results of the competition could answer interesting
questions related to what modality of data is the most useful, and if combining
modalities results in better performance. For example, we are given only 50
seconds of EEG recordings, but 10 minutes of spectrogram data (larger temporal
window, but at a lower resolution). Is it that the more global information from
the spectrograms, or high resolution information from the EEG signals, will give
the best results?</p>
<h3 id="the-labels">The Labels</h3>
<p>An important distinction to make is that although this competition is a
<em>classification</em> competition, we are not given categorical labels for each
sample. Instead, multiple expert annotators reviewed both the EEG and
spectrogram data, then independently voted on the correct label. Therefore, the
<em>label</em> that we are given are the votes of the multiple expert annotators, and
we are instead tasked with predicting the <strong>distribution</strong> of expert annotator
votes. To that end, we were scored with the KL Divergence metric, which is a
distance metric used to compare two distributions. The KL-Divergence metric is
computed as follows:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>L</mi><mo stretchy="false">(</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mtext>  </mtext><msub><mi>y</mi><mi>i</mi></msub><mo>∗</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><msub><mi>y</mi><mi>i</mi></msub><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub></mfrac><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
L(\hat{y}_i, y_i) &amp;= \sum_i \; y_i * log(\frac{y_i}{\hat{y}_i})  \\
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.6852em;vertical-align:-1.0926em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5926em"><span style="top:-3.5926em"><span class="pstrut" style="height:3.1076em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0926em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5926em"><span style="top:-3.5926em"><span class="pstrut" style="height:3.1076em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0926em"><span></span></span></span></span></span></span></span></span></span></span></span>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\hat{y}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> are the predicted and ground truth labels
respectively, which are normalized to have a sum equal to 1 in order to make
them proper distributions.</p>
<blockquote>
<p>What’s interesting about this metric, that may have unintended consequences,
is that it is not commutative. That is, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo mathvariant="normal">≠</mo><mi>k</mi><mo stretchy="false">(</mo><mi>b</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">k(a, b) \neq k(b, a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">k(x, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mclose">)</span></span></span></span>
is the KL-Divergence between the distributions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span>. For these reasons,
there were a couple comments on why other commutative metrics (such as
Jensen-Shannon Divergence) weren’t used instead.</p>
</blockquote>
<p>In the next 3 sections, we will explore important exploratory data analysis (EDA)
findings that played a key role in guiding certain parts of our solution.</p>
<h3 id="the-distribution-of-the-annotator-count-per-sample-is-bimodal">The Distribution of the Annotator Count per Sample is Bimodal</h3>
<p>Plotting the distribution of the number of annotators that voted on each sample
reveals a bimodal distribution (<strong>Figure 4</strong>). Looking closer at this distribution,
it turns out that there are samples where the annotator count is between 1-7 and
10-28, but there are <strong>no</strong> samples where the annotator count is 8 or 9.
This, I believe, hints at the true bimodality of the dataset.</p>
<p>Throughout the rest of this post, we will consider <strong>high quality</strong> samples to
be those where the annotator count is greater than or equal to 10 (G10).
Inversely, <strong>low quality</strong> samples are those where the annotator count is less
than 10 (L10).</p>
<center><figure><img src="/img/hbac/num_annotators.png" width="60%" style="background-color: white;"/><figcaption><p><b>Figure 4:</b> The distribution of the number of expert annotators per sample. Notice
that the distribution is bimodal, and that most samples contain 3 expert annotators.</p></figcaption></figure></center>
<p>During the earlier weeks of the competition, people noticed that the public LB
scores seemed to be significantly lower than local CV scores [<span class="citation-ref" data-citation-id="13" data-astro-cid-tbsazfha>13</span>]. After
some digging, it turns out that creating a validation set with only <em>high-quality</em>
samples results in an almost 1:1 relationship between local CV and public LB.
Given this information, there were a couple of extremely important things that
we noted at the time:</p>
<ol>
<li>The public LB <em>most likely</em> contains high-quality samples, where the
annotator count is greater than 9. Although we can’t know this for sure.</li>
<li>If the public LB is composed of high-quality samples, it could be that the
private LB has a larger subset of low-quality samples. If this is true, a big
public-to-private LB shakeup could be expected if people overfit the public LB.</li>
</ol>
<p>Some people [<span class="citation-ref" data-citation-id="14" data-astro-cid-tbsazfha>14</span>] believed that we would see a large shakeup due to
possibility number 2 listed above, and bet their whole solution on it. Because
we were allowed to submit two solutions, we played it safe and had a submission
optimized for each situation, as we’ll explain later.</p>
<h3 id="there-is-a-varying-level-of-annotator-agreement-per-sample">There is a Varying Level of Annotator Agreement Per Sample</h3>
<p>Given that we have multiple expert annotators reviewing the data, there are
cases of disagreement in the correct label. Visualizing the level of agreement
per sample reveals that most samples have 100% agreement, but that there are
plenty of cases in which disagreement is present (<strong>Figure 5</strong>).</p>
<center><figure><img src="/img/hbac/agreement.png" width="60%" style="background-color: white;"/><figcaption><p><b>Figure 5:</b> The distribution of annotator agreement per sample.</p></figcaption></figure></center>
<p>The hosts of the Kaggle competition gave a formal definition of the kinds of
annotator agreement/disagreement, these names being: Idealized, Proto, and
Edge. If your interested in reading more about how they define these, check
out <a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/overview" target="_blank">this link</a>.
Here, we will refer samples with at least 90% agreement as <strong>idealized</strong> samples,
and samples with less than 90% agreement as <strong>non-idealized</strong> samples.</p>
<h2 id="methodology">Methodology</h2>
<p>For this competition, we benchmarked three models: Two unimodal models trained on
the 1D EEG signals and the 2D spectrograms respectively, and a multimodal model
that extends and combines the two previous models into a single model. We claim
that the presented EEG model makes minimal assumptions about the temporal length
of the EEG, the number and spatial orientation of the EEG channels, and can
trivially be extended to include new sources and modalities of data.</p>
<h3 id="preprocessing">Preprocessing</h3>
<p>For preprocessing of the 1D EEG signals, we first computed the bipolar banana
montage (excluding the EKG, Cz-Fz, and Pz-Cz channels). Then, each bipolar
channel was temporally filtered with a Butterworth bandpass filter to remove
frequencies outside the 0.25 to 50 Hz band. The resulting signal was then
downsampled from 200 Hz to 50 Hz. Finally, we normalized the signal by the mean
absolute deviation (MAD), and clipped the values to be between -10 and 10 to
remove the presence of extreme values.</p>
<p>We chose to normalize by the MAD after finding the traditional standard
deviation to not be a robust normalization factor due to the presence of large
artifacts in some of EEG signals (<strong>Figure 6</strong>).</p>
<center><figure><img src="/img/hbac/artifact-single.png" width="90%" style="background-color: white;"/><figcaption><p><b>Figure 6</b>: An example of a channel with large artifacts.</p></figcaption></figure></center>
<p>For preprocessing of the 2D spectrograms, we clipped the raw spectrograms
between <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">e^{-4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mn>7</mn></msup></mrow><annotation encoding="application/x-tex">e^{7}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">7</span></span></span></span></span></span></span></span></span></span></span></span>, took the log, and then normalized each spectrogram
to have mean 0 and standard deviation 1.</p>
<h3 id="model-architecture">Model Architecture</h3>
<p>At the beginning of this competition, we had a couple of properties that we
wanted our model to satisfy. First, we would like it to not rely on the length
of the EEG signal to make it robust to novel contexts. Next, we would prefer
that it did not rely on the spatial orientation of the EEG electrodes, but could
learn these relationships from the data during training. Finally, we wanted the
model to work with minimally preprocessed data, and scale well.</p>
<p>To satisfy these properties, we took inspiration from some of the recent progress
made in natural language processing, and decided to treat each spectrogram and
EEG channel as a token that needs to be embedded into a single vector. After
embedding each channel, we used an additive attention layer with learned
positional encodings to route and mix the embeddings, effectively allowing us to
learn the relationships between the different electrode channels without encoding
it explicitly. In the last layer, we applied attention-pooling followed by a
linear layer and softmax for classification.</p>
<center><figure><img src="/img/hbac/multi-modal-model.png" width="100%" style="background-color: white;"/><figcaption><p><b>Figure 7:</b> The architecture of the 1D EEG model (1), the 2D
spectrogram model (2), and the multimodal model (3). As well as important
blocks used, such as the additive attention layer (4) and the decoder (5).</p></figcaption></figure></center>
<p>Applying this framework in practice leaves one question: How do we embed each
source of data into a vector in a manner that satisfies the properties previously
mentioned?</p>
<p>To embed the 1D EEG channels, we used a CNN-RNN architecture that first stacks
five 1D residual convolution blocks to simultaneously learn local features and
downsample the preprocessed 50 Hz EEG channel to ~1 Hz. Afterwards, we use a
recurrent neural network (RNN) within a many-to-one framework to temporally pool
the signal into a single vector embedding. We claim that the use of an RNN for
temporal pooling makes our model robust to the length of the EEG signal.</p>
<p>To embed the 2D spectrograms that are jointly provided with the EEG signals, we
used a basic CNN architecture with ResNet-like encoder blocks followed by global
average pooling. This architecture was again used to independently embed each of
the 4 spectrograms provided (LL, LP, RL, RP).</p>
<p>The final multimodal model is an extension of both of the above architectures
(see <strong>Figure 7</strong>). First, we take the embeddings of each EEG channel and
spectrogram out of the previously discussed models (after the final attention
layer, but before attention pooling), and then train an additional multi-layer
additive attention layer on top of these embeddings to attend to every signal.
We claim that this method could be modified to incorporate any modality of data,
including text, assuming that you have a method of embedding the new data.</p>
<h3 id="cross-validation-cv-setup">Cross-Validation (CV) Setup</h3>
<blockquote>
<p>Note: <code>CV</code> is slang used in Kaggle to represent how you compute your score
locally. Setting up a good local CV scheme is crucial to obtain a good score
in a competition, since it reduces the number of times that you have to submit
to the public LB. For example, if you had a local CV scheme that was perfectly
linear with the public LB score, then you would rarely need to submit to the
Kaggle public LB, and would essentially have infinite submissions. This allows
for rapid testing of new ideas.</p>
</blockquote>
<p>For our validation setup, we used a 10-fold GroupKFold on EEG ID, grouped by
patient ID. We chose GroupKFold with these parameters because it ensures that
out-of-fold (OOF) data only contains EEG recordings from patients who were not
in the training set. This is advantageous because we believe the true test of
generalizability is if a model can generalize to new patients. The testing set
was chosen as the public LB in the Kaggle competition to ensure good alignment
between local CV and public LB scores.</p>
<h3 id="training">Training</h3>
<p>For training, we developed a two-stage training scheme that takes advantage of
the bimodal nature of the annotator count per sample (see <strong>Data</strong>). For the
first stage, we trained on all of the data within the current fold for 20 epochs
using the Adam optimizer with a static learning rate of 0.0003. In the second
stage, we trained for 10 more epochs on just the high-quality samples, and
reduced the learning rate to 0.0001. Beyond statically reducing the learning
rate in the second stage, no learning rate schedulers were used. We used the
KL-Divergence loss function, since it’s the metric we’re being tested on.
Training was done in PyTorch on a NVIDIA GTX 1070 8 Gb GPU, and takes ~1 hour
per fold.  Therefore, the final submission took ~30 hours to train in total (3
models * 1 hour per model per fold * 10 folds).</p>
<p>For training augmentations on the 1D EEGs, we horizontally and vertically flip
(p = 0.5) the ordering of the channels in the banana montage. Notice that when
vertically flipping, you must also reverse the sign of the data because the
order of subtraction in the <em>bipolar</em> channel changed. We also randomly zeroed
out between 1-4 channels at a time with probability 0.5. For the 2D spectrograms,
we only swapped the direction of the chains (both ll with rl and lp with rp).</p>
<center><figure><img src="/img/hbac/train-loss.png" width="100%" style="background-color: white;"/><figcaption><p><b>Figure 8:</b> The training loss of each model. The dotted line represents the transition
from stage 1 to stage 2 of training. The high-quality and low-quality samples are plotted
separately.</p></figcaption></figure></center>
<p>Minimal hyperparameter tuning was done. When evaluated on the testing set,
single model performance demonstrated high variance, and so we chose an ensemble
of 10 folds to combat against this high variance. Training for 20 and 10 epochs were
conservatively chosen for stage 1 and 2 respectively, driven by earlier experimental
observations that models had a tendency to converge around ~10 epochs in stage 1
and ~3 epochs in stage 2.</p>
<h2 id="results">Results</h2>
<p>We evaluate the results of the three models presented above with the
KL-Divergence and AUC-ROC metrics, as shown in <strong>Table 1</strong>.</p>









































<table><thead><tr><th>Model</th><th>auc_all</th><th>auc_hq</th><th>auc_lq</th><th>kldiv_all</th><th>kldiv_hq</th><th>kldiv_lq</th></tr></thead><tbody><tr><td>1D EEG</td><td>0.9203 ± 0.0317</td><td>0.9896 ± 0.0080</td><td>0.9364 ± 0.0368</td><td>0.6527 ± 0.0194</td><td>0.2757 ± 0.0214</td><td>0.8367 ± 0.0209</td></tr><tr><td>2D Spectrogram</td><td>0.8731 ± 0.0422</td><td>0.9719 ± 0.0162</td><td>0.8866 ± 0.0612</td><td>0.8346 ± 0.0825</td><td>0.4284 ± 0.0278</td><td>1.0320 ± 0.1064</td></tr><tr><td>Multimodal</td><td>0.9274 ± 0.0272</td><td>0.9944 ± 0.0045</td><td>0.9408 ± 0.0330</td><td>0.6376 ± 0.0289</td><td>0.2625 ± 0.0233</td><td>0.8289 ± 0.0313</td></tr></tbody></table>
<p>In summary, the 1D EEG model was the best performing unimodal model achieving
an AUC Of 0.9203 ± 0.0317 on all of the idealized samples, and 0.9896 ± 0.0080 on
the high-quality idealized samples. The 1D EEG model far outperformed the 2D
spectrogram model on every metric, which achieved an AUC of 0.8731 ± 0.0422 on
all of the idealized samples, and 0.9896 ± 0.0080 on the high-quality idealized
samples. The multimodal outperformed both unimodal models, and was a small (but
significant) improvement over the 1D EEG model by 0.0071 AUC on all of the idealized
samples, and 0.0151 KL-Divergence on the high-quality samples.</p>
<center><figure><img src="/img/hbac/auc-all.png" width="100%" style="background-color: white;"/><figcaption><p><b>Figure 9:</b> AUC curves shown for each model, and for each type of harmful
brain activity. Results are only shown for idealized samples that had greater than
or equal to 3 expert annotator votes.</p></figcaption></figure></center>
<p>Interpreting model performance, it seems that the higher temporal resolution of
the 1D EEG model allowed it to outperform the 2D spectrogram model.  Although,
there also seems to be some amount of information contained within the 2D
spectrograms that is not present in the 1D EEG signals, allowing the multimodal
model to outperform both unimodal models.</p>
<center><figure><img src="/img/hbac/auc-multi-low-high.png" width="100%" style="background-color: white;"/><figcaption><p><b>Figure 10:</b> AUC curves from the multimodal model shown for each type of
harmful brain activity, separated between low-quality and high-quality
samples. Results only shown for idealized samples that had greater than or
equal to 3 expert annotator votes.</p></figcaption></figure></center>
<p>Note the large gap in performance between the high-quality samples and the
low-quality samples in the final evaluation and during training (<strong>Table 1</strong> and
<strong>Figure 9/10</strong>). We believe that this is for two reasons: First, there is more
uncertainty in the labels of the low-quality samples as there are less expert
annotators voting on the label. Second, the entropy of the ground truth labels
are different in the low and high quality samples. That is, the low-quality
samples are inherently low entropy because there is more sparsity in the labels,
whereas the high-quality samples are higher entropy and less sparse leading to a
distribution shift in the KL-Divergence metric. If the true cause of the difference
in performance between high and low quality labels is a distribution shift in the
entropy, one could likely fix this by adding a small constant epsilon to all of
the low-quality samples to match the average entropy of the high quality
samples. We did not test this.</p>
<p>For the kaggle competition, this solution ranked us 60th out of 2767 teams on
the private LB with a KL-Divergence score of 0.338933. For reference, the first
place winners (team Sony) won with a KL-Divergence score of 0.272332.
Post-competition, we did not witness a large public-to-private LB shakeup, which
we interpret to mean that both the public and private LB had a large sampling of
high-confidence labels. Although again, we can’t know this for sure.</p>
<h2 id="discussion">Discussion</h2>
<p>In this post we presented an attention-style model to perform classification of
harmful brain activity from EEG signals. We show that our multimodal model
achieves 0.9274 ± 0.0272 AUC on idealized samples and 0.2625 ± 0.0233
KL-Divergence on high-quality samples. Furthermore, we claimed that our 1D EEG
model is robust to the length of the EEG signal, the number of spatial
orientation of the EEG electrodes, works with minimally preprocessed data, and
scales well. With this model, we were able to rank 60th out of 2767 others teams
in the Kaggle competition.</p>
<p>Although, our model is not without limitations. Specifically, we acknowledge the
problem of interpretability. On this topic, we want to highlight the previous
work by Barnett et al. [<span class="citation-ref" data-citation-id="15" data-astro-cid-tbsazfha>15</span>] who proposed an interpretable solution that
allows models to express their predictions in the form of “this looks like
that”. We believe that this is a strong solution to the interpretability problem
within the context of brain activity classification, and could benefit care
teams.</p>
<p>In reading other competitors’ solutions [<span class="citation-ref" data-citation-id="16" data-astro-cid-tbsazfha>16</span>]  [<span class="citation-ref" data-citation-id="17" data-astro-cid-tbsazfha>17</span>]  [<span class="citation-ref" data-citation-id="18" data-astro-cid-tbsazfha>18</span>]  [<span class="citation-ref" data-citation-id="19" data-astro-cid-tbsazfha>19</span>]  [<span class="citation-ref" data-citation-id="20" data-astro-cid-tbsazfha>20</span>]  [<span class="citation-ref" data-citation-id="21" data-astro-cid-tbsazfha>21</span>]  [<span class="citation-ref" data-citation-id="22" data-astro-cid-tbsazfha>22</span>],
we realize that while our general architecture might be strong, the encoders
themselves are fairly weak. We hypothesize that one could see significant
improvements in performance by strengthening the encoders of both the 1D EEG and
2D spectrogram models.</p>
<p>We believe there are lots of room for improvement and new ideas, with some of
the bigger ones being:</p>
<ol>
<li>Experiment with pre-training on other datasets (unsupervised and supervised).</li>
<li>Changing the task from classification to a more real-time event-detection.</li>
<li>Develop a library for rapid benchmarking of various machine learning models and ideas.</li>
</ol>
<p><strong>Code and data availability:</strong> The dataset used is publicly available and can
be downloaded from the Kaggle website. The code for this project, and
instructions for full reproducibility, can be found on
<a href="https://github.com/ryanirl/hbac" target="_blank">GitHub</a>.
The final Kaggle notebook and models used can be found on
<a href="https://www.kaggle.com/code/ryanirl/hms-baseline-sub" target="_blank">Kaggle</a>.
All figures can also be reproduced (see <strong>Figures</strong> below).</p>
<div id="citations-data" style="display: none;" data-astro-cid-eqo3tkq5> [{&quot;id&quot;:1,&quot;title&quot;:&quot;An Intelligent EEG Classification Methodology Based on Sparse Representation Enhanced Deep Learning Networks&quot;,&quot;authors&quot;:[&quot;Huang, J.-S.&quot;,&quot;Li, Y.&quot;,&quot;Chen, B.-Q.&quot;,&quot;Lin, C.&quot;,&quot;Yao, B.&quot;],&quot;year&quot;:2020,&quot;url&quot;:&quot;https://doi.org/10.3389/fnins.2020.00808&quot;},{&quot;id&quot;:2,&quot;title&quot;:&quot;Electroencephalogram Signal Classification for Automated Epileptic Seizure Detection Using Genetic Algorithm&quot;,&quot;authors&quot;:[&quot;Nanthini, B. S.&quot;,&quot;Santhi, B.&quot;],&quot;year&quot;:2017,&quot;url&quot;:&quot;https://doi.org/10.4103/jnsbm.jnsbm_285_16&quot;},{&quot;id&quot;:3,&quot;title&quot;:&quot;EEG-GCNN: Augmenting Electroencephalogram-Based Neurological Disease Diagnosis Using a Domain-Guided Graph Convolutional Neural Network&quot;,&quot;authors&quot;:[&quot;Wagh, N.&quot;,&quot;Varatharajah, Y.&quot;],&quot;year&quot;:2020,&quot;url&quot;:&quot;https://doi.org/10.48550/arxiv.2011.12107&quot;},{&quot;id&quot;:4,&quot;title&quot;:&quot;EEG Classification Via Convolutional Neural Network-Based Interictal Epileptiform Event Detection&quot;,&quot;authors&quot;:[&quot;Thomas, J.&quot;,&quot;Comoretto, L.&quot;,&quot;Jin, J.&quot;,&quot;Dauwels, J.&quot;,&quot;Cash, S. S.&quot;,&quot;Westover, M. B.&quot;],&quot;year&quot;:2018,&quot;url&quot;:&quot;https://doi.org/10.1109/EMBC.2018.8512930&quot;},{&quot;id&quot;:5,&quot;title&quot;:&quot;The Role of EEG in the Diagnosis and Management of Patients with Sleep Disorders&quot;,&quot;authors&quot;:[&quot;Behzad, R. and Behzad, A.&quot;],&quot;year&quot;:2021,&quot;url&quot;:&quot;https://doi.org/10.4236/jbbs.2021.1110021&quot;},{&quot;id&quot;:6,&quot;title&quot;:&quot;Diagnosis of Alzheimer’s disease via resting-state EEG: integration of spectrum, complexity, and synchronization signal features&quot;,&quot;authors&quot;:[&quot;Zheng X, Wang B, Liu H, Wu W, Sun J, Fang W, Jiang R, Hu Y, Jin C, Wei X and Chen SS-C&quot;],&quot;year&quot;:2023,&quot;url&quot;:&quot;https://doi.org/10.3389/fnagi.2023.1288295&quot;},{&quot;id&quot;:7,&quot;title&quot;:&quot;HMS - Harmful Brain Activity Classification&quot;,&quot;authors&quot;:[&quot;Jing, J.&quot;,&quot;Lin, Z.&quot;,&quot;Yang, C.&quot;,&quot;Chow, A.&quot;,&quot;Dane, S.&quot;,&quot;Sun, J.&quot;,&quot;Westover, M. B.&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://kaggle.com/competitions/hms-harmful-brain-activity-classification&quot;},{&quot;id&quot;:8,&quot;title&quot;:&quot;Two-photon calcium imaging of neuronal activity&quot;,&quot;authors&quot;:[&quot;Grienberger, C., Giovannucci, A., Zeiger, W. et al.&quot;],&quot;year&quot;:2022,&quot;url&quot;:&quot;https://doi.org/10.1038/s43586-022-00147-1&quot;},{&quot;id&quot;:9,&quot;title&quot;:&quot;Large-Scale Fluorescence Calcium-Imaging Methods for Studies of Long-Term Memory in Behaving Mammals&quot;,&quot;authors&quot;:[&quot;Jercog, P., Rogerson, T., &amp; Schnitzer, M. J.&quot;],&quot;year&quot;:2016,&quot;url&quot;:&quot;https://doi.org/10.1101/cshperspect.a021824&quot;},{&quot;id&quot;:10,&quot;title&quot;:&quot;Electroencephalography (EEG): An Introductory Text and Atlas of Normal and Abnormal Findings in Adults, Children, and Infants&quot;,&quot;authors&quot;:[&quot;Britton JW, Frey LC, Hopp JLet al., authors; St. Louis EK, Frey LC&quot;],&quot;year&quot;:2016,&quot;url&quot;:&quot;https://www.ncbi.nlm.nih.gov/books/NBK390351/&quot;},{&quot;id&quot;:11,&quot;title&quot;:&quot;EEG-Based Sleep Staging Analysis with Functional Connectivity&quot;,&quot;authors&quot;:[&quot;Huang, H., Zhang, J., Zhu, L., Tang, J., Lin, G., Kong, W., Lei, X., &amp; Zhu, L.&quot;],&quot;year&quot;:2021,&quot;url&quot;:&quot;https://doi.org/10.3390/s21061988&quot;},{&quot;id&quot;:12,&quot;title&quot;:&quot;Why shake-up didn&#39;t happen&quot;,&quot;authors&quot;:[&quot;Evitan, G.&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492262&quot;},{&quot;id&quot;:13,&quot;title&quot;:&quot;Whats with the CV in this comp?&quot;,&quot;authors&quot;:[&quot;Null, C.&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/478277&quot;},{&quot;id&quot;:14,&quot;title&quot;:&quot;High Votes Distribution Wins!&quot;,&quot;authors&quot;:[&quot;Null, C.&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492190&quot;},{&quot;id&quot;:15,&quot;title&quot;:&quot;Improving Clinician Performance in Classifying EEG Patterns on the Ictal-Interictal Injury Continuum Using Interpretable Machine Learning&quot;,&quot;authors&quot;:[&quot;Barnett, A. J., Guo, Z., Jing, J., Ge, W., Kaplan, P. W., Kong, W. Y., Karakis, I., Herlopian, A., Jayagopal, L. A., Taraschenko, O., Selioutski, O., Osman, G., Goldenholz, D., Rudin, C., &amp; Westover, M. B.&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://doi.org/10.1056/aioa2300331&quot;},{&quot;id&quot;:16,&quot;title&quot;:&quot;1st place solution, team Sony&quot;,&quot;authors&quot;:[&quot;suguuuuu&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492560&quot;},{&quot;id&quot;:17,&quot;title&quot;:&quot;2nd place solution&quot;,&quot;authors&quot;:[&quot;COOLZ&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492254&quot;},{&quot;id&quot;:18,&quot;title&quot;:&quot;3rd place solution&quot;,&quot;authors&quot;:[&quot;Dieter&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492471&quot;},{&quot;id&quot;:19,&quot;title&quot;:&quot;4th place solution&quot;,&quot;authors&quot;:[&quot;YujiAriyasu&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492240&quot;},{&quot;id&quot;:20,&quot;title&quot;:&quot;6th Place Solution for the HMS - Harmful Brain Activity Classification Competition&quot;,&quot;authors&quot;:[&quot;Vu Minh Quan&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492619&quot;},{&quot;id&quot;:21,&quot;title&quot;:&quot;9th Place Solution&quot;,&quot;authors&quot;:[&quot;ishikei&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492429&quot;},{&quot;id&quot;:22,&quot;title&quot;:&quot;11th Place Solution&quot;,&quot;authors&quot;:[&quot;T88&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492301&quot;},{&quot;id&quot;:23,&quot;title&quot;:&quot;60th Place Solution: Single Multimodal Attention Model&quot;,&quot;authors&quot;:[&quot;Peters, R.&quot;],&quot;year&quot;:2024,&quot;url&quot;:&quot;https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492189&quot;}] </div> <div class="citation-tooltip" id="citationTooltip" data-astro-cid-eqo3tkq5></div> <h2 data-astro-cid-eqo3tkq5>References</h2> <ol class="references-list" data-astro-cid-eqo3tkq5> <li id="citation-1" data-astro-cid-eqo3tkq5> Huang, J.-S., Li, Y., Chen, B.-Q., Lin, C., Yao, B. (2020). An Intelligent EEG Classification Methodology Based on Sparse Representation Enhanced Deep Learning Networks.
<a href="https://doi.org/10.3389/fnins.2020.00808" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-2" data-astro-cid-eqo3tkq5> Nanthini, B. S., Santhi, B. (2017). Electroencephalogram Signal Classification for Automated Epileptic Seizure Detection Using Genetic Algorithm.
<a href="https://doi.org/10.4103/jnsbm.jnsbm_285_16" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-3" data-astro-cid-eqo3tkq5> Wagh, N., Varatharajah, Y. (2020). EEG-GCNN: Augmenting Electroencephalogram-Based Neurological Disease Diagnosis Using a Domain-Guided Graph Convolutional Neural Network.
<a href="https://doi.org/10.48550/arxiv.2011.12107" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-4" data-astro-cid-eqo3tkq5> Thomas, J., Comoretto, L., Jin, J., Dauwels, J., Cash, S. S., Westover, M. B. (2018). EEG Classification Via Convolutional Neural Network-Based Interictal Epileptiform Event Detection.
<a href="https://doi.org/10.1109/EMBC.2018.8512930" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-5" data-astro-cid-eqo3tkq5> Behzad, R. and Behzad, A. (2021). The Role of EEG in the Diagnosis and Management of Patients with Sleep Disorders.
<a href="https://doi.org/10.4236/jbbs.2021.1110021" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-6" data-astro-cid-eqo3tkq5> Zheng X, Wang B, Liu H, Wu W, Sun J, Fang W, Jiang R, Hu Y, Jin C, Wei X and Chen SS-C (2023). Diagnosis of Alzheimer’s disease via resting-state EEG: integration of spectrum, complexity, and synchronization signal features.
<a href="https://doi.org/10.3389/fnagi.2023.1288295" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-7" data-astro-cid-eqo3tkq5> Jing, J., Lin, Z., Yang, C., Chow, A., Dane, S., Sun, J., Westover, M. B. (2024). HMS - Harmful Brain Activity Classification.
<a href="https://kaggle.com/competitions/hms-harmful-brain-activity-classification" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-8" data-astro-cid-eqo3tkq5> Grienberger, C., Giovannucci, A., Zeiger, W. et al. (2022). Two-photon calcium imaging of neuronal activity.
<a href="https://doi.org/10.1038/s43586-022-00147-1" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-9" data-astro-cid-eqo3tkq5> Jercog, P., Rogerson, T., &amp; Schnitzer, M. J. (2016). Large-Scale Fluorescence Calcium-Imaging Methods for Studies of Long-Term Memory in Behaving Mammals.
<a href="https://doi.org/10.1101/cshperspect.a021824" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-10" data-astro-cid-eqo3tkq5> Britton JW, Frey LC, Hopp JLet al., authors; St. Louis EK, Frey LC (2016). Electroencephalography (EEG): An Introductory Text and Atlas of Normal and Abnormal Findings in Adults, Children, and Infants.
<a href="https://www.ncbi.nlm.nih.gov/books/NBK390351/" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-11" data-astro-cid-eqo3tkq5> Huang, H., Zhang, J., Zhu, L., Tang, J., Lin, G., Kong, W., Lei, X., &amp; Zhu, L. (2021). EEG-Based Sleep Staging Analysis with Functional Connectivity.
<a href="https://doi.org/10.3390/s21061988" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-12" data-astro-cid-eqo3tkq5> Evitan, G. (2024). Why shake-up didn&#39;t happen.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492262" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-13" data-astro-cid-eqo3tkq5> Null, C. (2024). Whats with the CV in this comp?.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/478277" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-14" data-astro-cid-eqo3tkq5> Null, C. (2024). High Votes Distribution Wins!.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492190" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-15" data-astro-cid-eqo3tkq5> Barnett, A. J., Guo, Z., Jing, J., Ge, W., Kaplan, P. W., Kong, W. Y., Karakis, I., Herlopian, A., Jayagopal, L. A., Taraschenko, O., Selioutski, O., Osman, G., Goldenholz, D., Rudin, C., &amp; Westover, M. B. (2024). Improving Clinician Performance in Classifying EEG Patterns on the Ictal-Interictal Injury Continuum Using Interpretable Machine Learning.
<a href="https://doi.org/10.1056/aioa2300331" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-16" data-astro-cid-eqo3tkq5> suguuuuu (2024). 1st place solution, team Sony.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492560" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-17" data-astro-cid-eqo3tkq5> COOLZ (2024). 2nd place solution.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492254" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-18" data-astro-cid-eqo3tkq5> Dieter (2024). 3rd place solution.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492471" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-19" data-astro-cid-eqo3tkq5> YujiAriyasu (2024). 4th place solution.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492240" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-20" data-astro-cid-eqo3tkq5> Vu Minh Quan (2024). 6th Place Solution for the HMS - Harmful Brain Activity Classification Competition.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492619" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-21" data-astro-cid-eqo3tkq5> ishikei (2024). 9th Place Solution.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492429" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-22" data-astro-cid-eqo3tkq5> T88 (2024). 11th Place Solution.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492301" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li><li id="citation-23" data-astro-cid-eqo3tkq5> Peters, R. (2024). 60th Place Solution: Single Multimodal Attention Model.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/492189" target="_blank" rel="noopener noreferrer" data-astro-cid-eqo3tkq5> Link</a> </li> </ol>  
<h2 id="figures">Figures</h2>
<ol>
<li>EEG electrodes.
<a href="https://www.mayoclinic.org/tests-procedures/eeg/about/pac-20393875" target="_blank">Link</a>.</li>
<li>10–20 system (EEG).
<a href="https://en.wikipedia.org/wiki/10%E2%80%9320_system_(EEG)" target="_blank">Link</a>.</li>
<li>Single sample of data.
<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/data" target="_blank">Link</a>.</li>
<li>Custom. Number of annotators.
<a href="https://github.com/ryanirl/blog-supplementary/blob/3e6ec63bee460b345b3499ffa1fdc71c2d94039e/hbac/figures.ipynb" target="_blank">Link</a>.</li>
<li>Custom. Distribution of agreement.
<a href="https://github.com/ryanirl/blog-supplementary/blob/3e6ec63bee460b345b3499ffa1fdc71c2d94039e/hbac/figures.ipynb" target="_blank">Link</a>.</li>
<li>Custom. Example of artifact.
<a href="https://github.com/ryanirl/blog-supplementary/blob/3e6ec63bee460b345b3499ffa1fdc71c2d94039e/hbac/figures.ipynb" target="_blank">Link</a>.</li>
<li>Custom. Model Architecture.</li>
<li>Custom. Training loss.
<a href="https://github.com/ryanirl/blog-supplementary/blob/3e6ec63bee460b345b3499ffa1fdc71c2d94039e/hbac/model_perf.ipynb" target="_blank">Link</a>.</li>
<li>Custom. Model performance (AUC).
<a href="https://github.com/ryanirl/blog-supplementary/blob/3e6ec63bee460b345b3499ffa1fdc71c2d94039e/hbac/model_perf.ipynb" target="_blank">Link</a>.</li>
<li>Custom. Multimodal model performance on high/low quality samples (AUC).
<a href="https://github.com/ryanirl/blog-supplementary/blob/3e6ec63bee460b345b3499ffa1fdc71c2d94039e/hbac/model_perf.ipynb" target="_blank">Link</a>.</li>
</ol>
<h2 id="tables">Tables</h2>
<ol>
<li>Custom.</li>
</ol>  </div> </div>  </div> </body></html>